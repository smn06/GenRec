{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8a8a0-27f9-48fe-84ac-f17197663297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from src.models.dqn_model import DQN  # Assuming DQN model is defined in src/models/dqn_model.py\n",
    "\n",
    "# Function to choose action based on epsilon-greedy policy\n",
    "def choose_action(state, epsilon, dqn, device):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.uniform(1, 5)  # Random rating between 1 and 5\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    q_values = dqn(state)\n",
    "    return q_values.cpu().detach().numpy()[0]\n",
    "\n",
    "# Function to preprocess state\n",
    "def preprocess_state(state, scaler):\n",
    "    return scaler.transform(state.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Function to train RL agent\n",
    "def train_rl(train_df_path, output_model_path, input_dim, num_episodes=1000, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=64, memory_size=10000):\n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the processed training data\n",
    "    train_df = pd.read_csv(train_df_path)\n",
    "\n",
    "    # Initialize the DQN, memory, and optimizer\n",
    "    dqn = DQN(input_dim=input_dim - 1, output_dim=1).to(device)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize MinMaxScaler for state normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data_scaled = scaler.fit_transform(train_df.iloc[:, :-1].values)\n",
    "\n",
    "    # Training the RL Agent\n",
    "    for episode in range(num_episodes):\n",
    "        state = train_data_scaled[np.random.randint(0, len(train_data_scaled))]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, dqn, device)\n",
    "            actual_rating = train_df.iloc[np.argmax(state), -1]  # Use the actual rating as the target for training\n",
    "            reward = -abs(actual_rating - action)  # Reward based on the closeness of the action to the actual rating\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = train_data_scaled[np.random.randint(0, len(train_data_scaled))]\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                actions = torch.FloatTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "                next_states = torch.FloatTensor(next_states).to(device)\n",
    "                dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "                q_values = dqn(states)\n",
    "                q_value = q_values.gather(1, actions.long())\n",
    "\n",
    "                next_q_values = dqn(next_states)\n",
    "                next_q_value = next_q_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "                expected_q_value = rewards + (gamma * next_q_value * (1 - dones))\n",
    "\n",
    "                loss = criterion(q_value, expected_q_value.detach())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    # Save the trained DQN model\n",
    "    torch.save(dqn.state_dict(), output_model_path)\n",
    "\n",
    "    print(f\"Training complete. Model saved to '{output_model_path}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    train_df_path = '../data/processed/train_user_profiles.csv'\n",
    "    output_model_path = '../models/dqn.pth'\n",
    "    input_dim = 5  # Example input dimension, adjust according to your data\n",
    "\n",
    "    train_rl(train_df_path, output_model_path, input_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
